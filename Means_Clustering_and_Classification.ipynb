{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNezth2zWYS6gcSNzELpawk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnRuskinONLINE/epic-classification/blob/master/Means_Clustering_and_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xnL8Cbr-tyco"
      },
      "outputs": [],
      "source": [
        "########################################################################################################\n",
        "#                            K-MEANS IMPLEMENTATION ON OPTICAL DATA                                    #\n",
        "# K-means implementation where the below algorithm will assign two classes to the Sentinel-2 optical   #\n",
        "# data that you have previously co-located. Two classes representing sea-ice, and lead, with a         #\n",
        "# possible third class representing no-data, if this is the case. Satellite data is rarely a nice      #\n",
        "# well-orientated shape!                                                                               #\n",
        "########################################################################################################\n",
        "\n",
        "!pip install rasterio\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You need to specify the path, this should be the same path that you saved your Sentinel-2 optical data.\n",
        "# Within the folder produced this will be within S2A_.../GRANULE/L1C.../IMG_DATA/\n",
        "base_path = \"directory\"\n",
        "bands_paths = {\n",
        "    'B4': base_path + 'T01WCU_20190301T235611_B04.jp2',\n",
        "    'B3': base_path + 'T01WCU_20190301T235611_B03.jp2',\n",
        "    'B2': base_path + 'T01WCU_20190301T235611_B02.jp2'\n",
        "}\n",
        "\n",
        "# Read and stack the band images\n",
        "band_data = []\n",
        "for band in ['B4', 'B3', 'B2']:\n",
        "    with rasterio.open(bands_paths[band]) as src:\n",
        "        band_data.append(src.read(1))\n",
        "\n",
        "# Stack bands and create a mask for valid data (non-zero values in all bands)\n",
        "band_stack = np.dstack(band_data)\n",
        "valid_data_mask = np.all(band_stack > 0, axis=2)\n",
        "\n",
        "# Reshape for K-means, only including valid data\n",
        "X = band_stack[valid_data_mask].reshape((-1, 3))\n",
        "\n",
        "# K-means clustering, we are only defining two clusters (ice and lead), but for other applications this could be more!\n",
        "kmeans = KMeans(n_clusters=2, random_state=0).fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "# Create an empty array for the result, filled with a no-data value (e.g., -1)\n",
        "labels_image = np.full(band_stack.shape[:2], -1, dtype=int)\n",
        "\n",
        "# Place cluster labels in the locations corresponding to valid data\n",
        "labels_image[valid_data_mask] = labels\n",
        "\n",
        "# Plotting the result\n",
        "plt.imshow(labels_image, cmap='viridis')\n",
        "plt.title('K-means clustering on Sentinel-2 Bands')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypJTbrditycp"
      },
      "outputs": [],
      "source": [
        "########################################################################################################\n",
        "#                               GMM-MEANS CLUSTERING ON OPTICAL DATA                                   #\n",
        "# GMM-means implementation where the below algorithm will assign two classes to the Sentinel-2 optical #\n",
        "# data that you have previously co-located. Two classes representing sea-ice, and lead, with a         #\n",
        "# possible third class representing no-data, if this is the case. Satellite data is rarely a nice      #\n",
        "# well-orientated shape!                                                                               #\n",
        "########################################################################################################\n",
        "\n",
        "!pip install rasterio\n",
        "import rasterio\n",
        "import numpy as np\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# You need to specify the path, this should be the same path that you saved your Sentinel-2 optical data.\n",
        "# Within the folder produced this will be within S2A_.../GRANULE/L1C.../IMG_DATA/\n",
        "base_path = \"directory\"\n",
        "bands_paths = {\n",
        "    'B4': base_path + 'T01WCU_20190301T235611_B04.jp2',\n",
        "    'B3': base_path + 'T01WCU_20190301T235611_B03.jp2',\n",
        "    'B2': base_path + 'T01WCU_20190301T235611_B02.jp2'\n",
        "}\n",
        "\n",
        "# Read and stack the band images\n",
        "band_data = []\n",
        "for band in ['B4', 'B3', 'B2']:\n",
        "    with rasterio.open(bands_paths[band]) as src:\n",
        "        band_data.append(src.read(1))\n",
        "\n",
        "# Stack bands and create a mask for valid data (non-zero values in all bands)\n",
        "band_stack = np.dstack(band_data)\n",
        "valid_data_mask = np.all(band_stack > 0, axis=2)\n",
        "\n",
        "# Reshape for GMM, only including valid data\n",
        "X = band_stack[valid_data_mask].reshape((-1, 3))\n",
        "\n",
        "# GMM clustering, again only defining two clusters for the AI to assign to\n",
        "gmm = GaussianMixture(n_components=2, random_state=0).fit(X)\n",
        "labels = gmm.predict(X)\n",
        "\n",
        "# Create an empty array for the result, filled with a no-data value (e.g., -1)\n",
        "labels_image = np.full(band_stack.shape[:2], -1, dtype=int)\n",
        "\n",
        "# Place GMM labels in the locations corresponding to valid data\n",
        "labels_image[valid_data_mask] = labels\n",
        "\n",
        "# Plotting the result\n",
        "plt.imshow(labels_image, cmap='viridis')\n",
        "plt.title('GMM clustering on Sentinel-2 Bands')\n",
        "plt.colorbar(label='Cluster Label')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAbIlEomtycq"
      },
      "outputs": [],
      "source": [
        "########################################################################################################\n",
        "#                                     ALTIMETRY CLASSIFICATION                                         #\n",
        "# Before delving into the modeling process, it's crucial to preprocess the data to ensure compatibility#\n",
        "# with our analytical models. This involves transforming the raw data into meaningful variables, such  #\n",
        "# as peakniness and stack standard deviation (SSD), etc.                                               #\n",
        "########################################################################################################\n",
        "\n",
        "# Importing important functions:\n",
        "\n",
        "!pip install netCDF4\n",
        "!pip install basemap\n",
        "!pip install cartopy\n",
        "from netCDF4 import Dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import griddata\n",
        "from mpl_toolkits.basemap import Basemap\n",
        "import numpy.ma as ma\n",
        "import glob\n",
        "from matplotlib.patches import Polygon\n",
        "import scipy.spatial as spatial\n",
        "from scipy.spatial import KDTree\n",
        "\n",
        "import pyproj\n",
        "import cartopy.crs as ccrs\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.cluster.hierarchy import linkage, fcluster\n",
        "\n",
        "# This code is very complicated, and for the running of the interesting bits does not particularly need to be understood,\n",
        "# the takeaway, insomuch as there is a takeaway, is that file types are being changed and reformatted.\n",
        "\n",
        "#=========================================================================================================\n",
        "#===================================  SUBFUNCTIONS  ======================================================\n",
        "#=========================================================================================================\n",
        "\n",
        "#*args and **kwargs allow you to pass an unspecified number of arguments to a function,\n",
        "#so when writing the function definition, you do not need to know how many arguments will be passed to your function\n",
        "#**kwargs allows you to pass keyworded variable length of arguments to a function.\n",
        "#You should use **kwargs if you want to handle named arguments in a function.\n",
        "#double star allows us to pass through keyword arguments (and any number of them).\n",
        "def peakiness(waves, **kwargs):\n",
        "\n",
        "    \"finds peakiness of waveforms.\"\n",
        "\n",
        "    #print(\"Beginning peakiness\")\n",
        "    # Kwargs are:\n",
        "    #          wf_plots. specify a number n: wf_plots=n, to show the first n waveform plots. \\\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "\n",
        "    print(\"Running peakiness function...\")\n",
        "\n",
        "    size=np.shape(waves)[0] #.shape property is a tuple of length .ndim containing the length of each dimensions\n",
        "                            #Tuple of array dimensions.\n",
        "\n",
        "    waves1=np.copy(waves)\n",
        "\n",
        "    if waves1.ndim == 1: #number of array dimensions\n",
        "        print('only one waveform in file')\n",
        "        waves2=waves1.reshape(1,np.size(waves1)) #numpy.reshape(a, newshape, order='C'), a=array to be reshaped\n",
        "        waves1=waves2\n",
        "\n",
        "    # *args is used to send a non-keyworded variable length argument list to the function\n",
        "    def by_row(waves, *args):\n",
        "        \"calculate peakiness for each waveform\"\n",
        "        maximum=np.nanmax(waves)\n",
        "        if maximum > 0:\n",
        "\n",
        "            maximum_bin=np.where(waves==maximum)\n",
        "            #print(maximum_bin)\n",
        "            maximum_bin=maximum_bin[0][0]\n",
        "            waves_128=waves[maximum_bin-50:maximum_bin+78]\n",
        "\n",
        "            waves=waves_128\n",
        "\n",
        "            noise_floor=np.nanmean(waves[10:20])\n",
        "            where_above_nf=np.where(waves > noise_floor)\n",
        "\n",
        "            if np.shape(where_above_nf)[1] > 0:\n",
        "                maximum=np.nanmax(waves[where_above_nf])\n",
        "                total=np.sum(waves[where_above_nf])\n",
        "                mean=np.nanmean(waves[where_above_nf])\n",
        "                peaky=maximum/mean\n",
        "\n",
        "            else:\n",
        "                peaky = np.nan\n",
        "                maximum = np.nan\n",
        "                total = np.nan\n",
        "\n",
        "        else:\n",
        "            peaky = np.nan\n",
        "            maximum = np.nan\n",
        "            total = np.nan\n",
        "\n",
        "        if 'maxs' in args:\n",
        "            return maximum\n",
        "        if 'totals' in args:\n",
        "            return total\n",
        "        if 'peaky' in args:\n",
        "            return peaky\n",
        "\n",
        "    peaky=np.apply_along_axis(by_row, 1, waves1, 'peaky') #numpy.apply_along_axis(func1d, axis, arr, *args, **kwargs)\n",
        "\n",
        "    if 'wf_plots' in kwargs:\n",
        "        maximums=np.apply_along_axis(by_row, 1, waves1, 'maxs')\n",
        "        totals=np.apply_along_axis(by_row, 1, waves1, 'totals')\n",
        "\n",
        "        for i in range(0,kwargs['wf_plots']):\n",
        "            if i == 0:\n",
        "                print(\"Plotting first \"+str(kwargs['wf_plots'])+\" waveforms\")\n",
        "\n",
        "            plt.plot(waves1[i,:])#, a, col[i],label=label[i])\n",
        "            plt.axhline(maximums[i], color='green')\n",
        "            plt.axvline(10, color='r')\n",
        "            plt.axvline(19, color='r')\n",
        "            plt.xlabel('Bin (of 256)')\n",
        "            plt.ylabel('Power')\n",
        "            plt.text(5,maximums[i],\"maximum=\"+str(maximums[i]))\n",
        "            plt.text(5,maximums[i]-2500,\"total=\"+str(totals[i]))\n",
        "            plt.text(5,maximums[i]-5000,\"peakiness=\"+str(peaky[i]))\n",
        "            plt.title('waveform '+str(i)+' of '+str(size)+'\\n. Noise floor average taken between red lines.')\n",
        "            plt.show()\n",
        "\n",
        "\n",
        "    return peaky\n",
        "\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "\n",
        "\n",
        "def unpack_gpod(variable):\n",
        "\n",
        "    from scipy.interpolate import interp1d\n",
        "\n",
        "    time_1hz=SAR_data.variables['time_01'][:]\n",
        "    time_20hz=SAR_data.variables['time_20_ku'][:]\n",
        "    time_20hzC = SAR_data.variables['time_20_c'][:]\n",
        "\n",
        "    out=(SAR_data.variables[variable][:]).astype(float)  # convert from integer array to float.\n",
        "\n",
        "    #if ma.is_masked(dataset.variables[variable][:]) == True:\n",
        "    #print(variable,'is masked. Removing mask and replacing masked values with nan')\n",
        "    out=np.ma.filled(out, np.nan)\n",
        "\n",
        "    if len(out)==len(time_1hz):\n",
        "\n",
        "        print(variable,'is 1hz. Expanding to 20hz...')\n",
        "        out = interp1d(time_1hz,out,fill_value=\"extrapolate\")(time_20hz)\n",
        "\n",
        "    if len(out)==len(time_20hzC):\n",
        "        print(variable, 'is c band, expanding to 20hz ku band dimension')\n",
        "        out = interp1d(time_20hzC,out,fill_value=\"extrapolate\")(time_20hz)\n",
        "    return out\n",
        "\n",
        "\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "#=========================================================================================================\n",
        "\n",
        "def calculate_SSD(RIP):\n",
        "\n",
        "    from scipy.optimize import curve_fit\n",
        "    from scipy import asarray as ar,exp\n",
        "    do_plot='Off'\n",
        "\n",
        "    def gaussian(x,a,x0,sigma):\n",
        "            return a * np.exp(-(x - x0)**2 / (2 * sigma**2))\n",
        "\n",
        "    SSD=np.zeros(np.shape(RIP)[0])*np.nan\n",
        "    x=np.arange(np.shape(RIP)[1])\n",
        "\n",
        "    for i in range(np.shape(RIP)[0]):\n",
        "\n",
        "        y=np.copy(RIP[i])\n",
        "        y[(np.isnan(y)==True)]=0\n",
        "\n",
        "        if 'popt' in locals():\n",
        "            del(popt,pcov)\n",
        "\n",
        "        SSD_calc=0.5*(np.sum(y**2)*np.sum(y**2)/np.sum(y**4))\n",
        "        #print('SSD calculated from equation',SSD)\n",
        "\n",
        "        #n = len(x)\n",
        "        mean_est = sum(x * y) / sum(y)\n",
        "        sigma_est = np.sqrt(sum(y * (x - mean_est)**2) / sum(y))\n",
        "        #print('est. mean',mean,'est. sigma',sigma_est)\n",
        "\n",
        "        try:\n",
        "            popt,pcov = curve_fit(gaussian, x, y, p0=[max(y), mean_est, sigma_est],maxfev=10000)\n",
        "        except RuntimeError as e:\n",
        "            print(\"Gaussian SSD curve-fit error: \"+str(e))\n",
        "            #plt.plot(y)\n",
        "            #plt.show()\n",
        "\n",
        "        except TypeError as t:\n",
        "            print(\"Gaussian SSD curve-fit error: \"+str(t))\n",
        "\n",
        "        if do_plot=='ON':\n",
        "\n",
        "            plt.plot(x,y)\n",
        "            plt.plot(x,gaussian(x,*popt),'ro:',label='fit')\n",
        "            plt.axvline(popt[1])\n",
        "            plt.axvspan(popt[1]-popt[2], popt[1]+popt[2], alpha=0.15, color='Navy')\n",
        "            plt.show()\n",
        "\n",
        "            print('popt',popt)\n",
        "            print('curve fit SSD',popt[2])\n",
        "\n",
        "        if 'popt' in locals():\n",
        "            SSD[i]=abs(popt[2])\n",
        "\n",
        "\n",
        "    return SSD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqjt8-nTtycr"
      },
      "outputs": [],
      "source": [
        "# This block of code is using the functions we have just defined to generate a large set of variables we will\n",
        "# utilise when plotting our altimetry data\n",
        "\n",
        "path = 'DIR' # The directory you saved to during the colocation code\n",
        "SAR_file='SAR' # The large file you downloaded from colocation, should begin with S3A, S3B, etc.\n",
        "print('overlapping SAR file is',SAR_file)\n",
        "SAR_data=Dataset(path + SAR_file+'/enhanced_measurement.nc')\n",
        "\n",
        "SAR_lat, SAR_lon, waves, sig_0, RIP= unpack_gpod('lat_20_ku'), unpack_gpod('lon_20_ku'), unpack_gpod('waveform_20_ku'),unpack_gpod('sig0_water_20_ku'),unpack_gpod('rip_20_ku')#unpack_gpod('Sigma0_20Hz')\n",
        "SAR_index=np.arange(np.size(SAR_lat))\n",
        "\n",
        "find=np.where(SAR_lat >= -99999)\n",
        "SAR_lat=SAR_lat[find]\n",
        "SAR_lon=SAR_lon[find]\n",
        "SAR_index=SAR_index[find]\n",
        "waves=waves[find]\n",
        "sig_0=sig_0[find]\n",
        "RIP=RIP[find]\n",
        "\n",
        "PP=peakiness(waves)\n",
        "SSD=calculate_SSD(RIP)\n",
        "sig_0_np = np.array(sig_0)\n",
        "RIP_np = np.array(RIP)\n",
        "PP_np = np.array(PP)\n",
        "SSD_np = np.array(SSD)\n",
        "\n",
        "data = np.column_stack((sig_0_np,PP_np, SSD_np))\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "data_normalized = scaler.fit_transform(data)\n",
        "\n",
        "# Then we delete any NaN (read: none, ?, lack thereof) values from the dataset\n",
        "nan_count = np.isnan(data_normalized).sum()\n",
        "print(f\"Number of NaN values in the array: {nan_count}\")\n",
        "data_cleaned = data_normalized[~np.isnan(data_normalized).any(axis=1)] # Now the data is cleaned!\n",
        "flag_cleaned = flag[~np.isnan(data_normalized).any(axis=1)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here, we first run our gaussian clustering model, having again defined two clusters: sea-ice and lead\n",
        "\n",
        "gmm = GaussianMixture(n_components=2, random_state=0)\n",
        "gmm.fit(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])\n",
        "clusters_gmm = gmm.predict(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])\n",
        "\n",
        "# Clean up the data:\n",
        "\n",
        "waves_cleaned=waves[~np.isnan(data_normalized).any(axis=1)][(flag_cleaned==1)|(flag_cleaned==2)]\n",
        "\n",
        "# And plot it!\n",
        "\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==0],axis=0),label='ice')\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==1],axis=0),label='lead')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "_G8zPVSKxhfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# You can also run our GMM-means model with more than two defined components, producing a graph that\n",
        "# represents roughness in sea-ice, where the wave-forms produced can be measured against values from the\n",
        "# literature to characterise whether they are slush, rough-ice, etcera\n",
        "\n",
        "gmm = GaussianMixture(n_components=5, random_state=0)\n",
        "gmm.fit(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])\n",
        "clusters_gmm = gmm.predict(data_cleaned[(flag_cleaned==1)|(flag_cleaned==2)])\n",
        "\n",
        "# Plot it again!\n",
        "\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==0],axis=0),label='class0')\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==1],axis=0),label='class1')\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==2],axis=0),label='class2')\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==3],axis=0),label='class3')\n",
        "plt.plot(np.mean(waves_cleaned[clusters_gmm==4],axis=0),label='class4')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "A69fxVsXyYCZ"
      },
      "execution_count": 1,
      "outputs": []
    }
  ]
}